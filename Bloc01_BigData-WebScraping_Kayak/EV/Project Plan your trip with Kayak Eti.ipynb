{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate jupyter: Select an Interpreter to start Jupyter\n",
      "Run the following command to install 'jupyter and notebook' into the Python environment. \n",
      "Command: 'python -m pip install jupyter notebook -U\n",
      "or\n",
      "conda install jupyter notebook -U'\n",
      "Click <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "import boto3\n",
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "liste_ville = []\n",
    "html_content = requests.get('https://one-week-in.com/35-cities-to-visit-in-france/').text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "for x in range(6,41):\n",
    "    liste_ville.append(soup.select(\"div.entry-content a\")[x].get_text())\n",
    "print(liste_ville)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Creation d'un Dataframe avec les noms de villes\n",
    "df=pd.DataFrame(data= {'Country':liste_ville})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Initialisation des listes lat et lon qui prendront les coordonnées des villes\n",
    "lat =[]\n",
    "lon=[]\n",
    "#Loop qui prend pour les 35 pays les lat et lon\n",
    "for i in tqdm(range(0,35)):\n",
    "    param= {'country' : 'France','q':liste_ville[i],'format':'json'}\n",
    "    r = requests.get(\"https://nominatim.openstreetmap.org/search?\",param)\n",
    "    lat.append(r.json()[0]['lat'])\n",
    "    lon.append(r.json()[0]['lon'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data= {'id': range(0,35) ,'city':liste_ville,'latitude':lat,'longitude':lon})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_meteo= pd.DataFrame()\n",
    "api_key='eb0e4bd4076bf5b895e823c02eb3e0d0'\n",
    "\n",
    "for x in tqdm(range(0,35)):\n",
    "\n",
    "    #Iteration sur les lat et lon de chaque ville pour interroger l'API\n",
    "    param= {'lat' : lat[x],'lon':lon[x],'appid' : api_key,'units':'metric'}#iteration sur les listes de cordonnées\n",
    "    r = requests.get(\"https://api.openweathermap.org/data/2.5/onecall?\",param)\n",
    "\n",
    "    for v in range (0,7) :#Pour des coordonnées precises --> recuperation des previsions meteo des 7 jours\n",
    "\n",
    "        extract_dict = r.json()['daily'][v]\n",
    "        #Suppression de quelque keys pas utile.\n",
    "        del extract_dict['feels_like']\n",
    "        del extract_dict['temp']\n",
    "        del extract_dict['weather']\n",
    "        #Ajout du nom de la ville en question dans la loop\n",
    "        extract_dict['city']=liste_ville[x]\n",
    "        #Ajout du id par ville\n",
    "        extract_dict['id_city']=x\n",
    "\n",
    "        df_meteo=df_meteo.append(extract_dict,ignore_index=True)\n",
    "\n",
    "\n",
    "df_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#OPERATION SUR COLONNES DATAFRAME\n",
    "df_meteo=df_meteo[['city','id_city','rain','pop','pressure','humidity','dt']]#Selection des colonnes utiles\n",
    "df_meteo['rain']=df_meteo['rain'].fillna(0)#Remplacement des data NaN en 0\n",
    "#Preciser les types de valeurs par colonnes\n",
    "df_meteo['id_city']=df_meteo['id_city'].astype(int)\n",
    "df['latitude']=df['latitude'].astype(float)\n",
    "df['longitude']=df['longitude'].astype(float)\n",
    "#Creation d'une colonne date issue de la colonne 'dt'\n",
    "df_meteo['dt_normal']= df_meteo['dt'].apply(lambda x : time.strftime('%d/%m/%Y %H:%M:%S',  time.gmtime(x)))\n",
    "df_meteo['dt_normal']= df_meteo['dt_normal'].apply(lambda x : pd.to_datetime(x))\n",
    "#Chargement des fichier en local\n",
    "df.to_csv('Info ville.csv')\n",
    "df_meteo.to_csv('Info meteo.csv')\n",
    "df_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Dataframe fusionnant des infos des 2 dataframe pour utilisation de mapbox\n",
    "df_propa_precipitation_per_city = df_meteo.groupby(['id_city','city']).agg({'rain': 'sum', 'pop':'sum'})\n",
    "df_propa_precipitation_per_city=df_propa_precipitation_per_city.reset_index()\n",
    "df_propa_precipitation_per_city['total_prediction_rain_per_city']=df_propa_precipitation_per_city['rain']+df_propa_precipitation_per_city['pop']\n",
    "df_propa_precipitation_per_city['latitude']=df['latitude']\n",
    "df_propa_precipitation_per_city['longitude']=df['longitude']\n",
    "df_propa_precipitation_per_city=df_propa_precipitation_per_city.sort_values('total_prediction_rain_per_city')\n",
    "df_propa_precipitation_per_city.to_csv('df_propa_precipitation_per_city.csv')\n",
    "df_propa_precipitation_per_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "destination_name = df_propa_precipitation_per_city.iloc[0,1]\n",
    "destination_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HotelsSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"hotels\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.booking.com/index.fr.html']\n",
    "\n",
    "    # Parse function for login\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to login\n",
    "\n",
    "\n",
    "            return scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata={'ss': destination_name},\n",
    "                callback=self.after_search\n",
    "            )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_search(self, response):\n",
    "\n",
    "        hotels = response.css('.sr_item')\n",
    "\n",
    "        for h in hotels:\n",
    "            yield {\n",
    "                'name': h.css('.sr-hotel__name::text').get(),\n",
    "                'url': \"https://www.booking.com\" + h.css('.hotel_name_link').attrib[\"href\"],\n",
    "                'coords': h.css('.sr_card_address_line a').attrib[\"data-coords\"],\n",
    "                'score': h.css('.bui-review-score__badge::text').get(),\n",
    "                'description': h.css('.hotel_desc::text').get()\n",
    "\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('a.paging-next').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"2_hotels.json\"\n",
    "\n",
    "if filename in os.listdir('/Users/alexlasnier/Downloads/Data_Collection_and_Management_Project-master/'):\n",
    "    os.remove( '/Users/alexlasnier/Downloads/Data_Collection_and_Management_Project-master/'+filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.ERROR,\n",
    "    \"FEEDS\": {\n",
    "        '/Users/alexlasnier/Downloads/Data_Collection_and_Management_Project-master/'+filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(HotelsSpider)\n",
    "\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Extract du fichier enregistré + nettoyage des colonnes\n",
    "data_booking=pd.read_json('src/2_hotels.json')\n",
    "\n",
    "data_booking['name']= data_booking['name'].apply(lambda x : x.replace('\\n',''))\n",
    "data_booking['description']= data_booking['description'].apply(lambda x : x.replace('\\n',''))\n",
    "data_booking['longitude'] = data_booking['coords'].apply(lambda x : x.split(',')[0])\n",
    "data_booking['latitude']=data_booking['coords'].apply(lambda x : x.split(',')[1])\n",
    "del data_booking['coords']\n",
    "data_booking=data_booking.dropna()\n",
    "data_booking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Connection a S3\n",
    "session = boto3.Session(aws_access_key_id=open('aws_access_key_id.txt').read(),\n",
    "                        aws_secret_access_key=open('aws_secret_access_key.txt').read())\n",
    "s3 = session.resource(\"s3\")\n",
    "s3_client = session.client('s3')\n",
    "new_buck = s3.create_bucket(Bucket=\"datalake-resort972\")\n",
    "#Key=nom a la destination  Body = Nom en local\n",
    "#Envoi de fichier sur s3\n",
    "my_buck = \"datalake-resort972\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# j'ai en local une extraction de srapy contenant des infos sur tout les hotels des 35 villes que je nettoie...\n",
    "all_city = pd.DataFrame()\n",
    "for ville  in liste_ville:\n",
    "    hotel_ville = pd.read_json('json\\hotels_'+ville+'.json')\n",
    "    hotel_ville['name']= hotel_ville['name'].apply(lambda x : x.replace('\\n',''))\n",
    "    hotel_ville['description']= hotel_ville['description'].apply(lambda x : x.replace('\\n',''))\n",
    "    hotel_ville['longitude'] = hotel_ville['coords'].apply(lambda x : x.split(',')[0])\n",
    "    hotel_ville['latitude']=hotel_ville['coords'].apply(lambda x : x.split(',')[1])\n",
    "    del hotel_ville['coords']\n",
    "    del hotel_ville['url']\n",
    "    del hotel_ville['description']\n",
    "    hotel_ville['city']=ville\n",
    "    new_buck.put_object(Key = 'hotel'+ville)\n",
    "    all_city=all_city.append(hotel_ville)\n",
    "    all_city = all_city=all_city.dropna()\n",
    "all_city.to_csv('hostel_data_set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Envoi des fichiers des dataframes informations d'hotel + prevision meteo\n",
    "s3_client.upload_file('hostel_data_set.csv', my_buck, 'hostel_data_set.csv')\n",
    "s3_client.upload_file('df_propa_precipitation_per_city.csv', my_buck, 'df_propa_precipitation_per_city.csv')\n",
    "\n",
    "#Telecharger tout les fichers necessaires depuis s3 bucket\n",
    "s3_client.download_file('datalake-resort972','hostel_data_set.csv','hostel_data_set.csv')\n",
    "s3_client.download_file('datalake-resort972','df_propa_precipitation_per_city.csv','df_propa_precipitation_per_city.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Connection a la BD\n",
    "dbuser ='postgres'\n",
    "dbpass ='Etienne972'\n",
    "dbhost ='database-1.cbwhzj6hzyst.us-east-1.rds.amazonaws.com'\n",
    "dbname ='postgres'\n",
    "engine =create_engine( f'postgresql+psycopg2://{dbuser}:{dbpass}@{dbhost}/{dbname}',echo=True)\n",
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Post sur PostgresSQL des dataframe\n",
    "all_city.to_sql(\n",
    "    'hostel',\n",
    "    engine\n",
    ")\n",
    "pd.read_csv('df_propa_precipitation_per_city.csv').to_sql(\n",
    "    \"propa_precipitation_per_city\",\n",
    "    engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Requete TOP 5 destination\n",
    "stmt = text(\"select city, total_prediction_rain_per_city,longitude, latitude \"\n",
    "            \"from propa_precipitation_per_city\"\n",
    "            \" order by total_prediction_rain_per_city ASC\"\n",
    "            \" limit 5;\")\n",
    "best_place = pd.read_sql(\n",
    "    stmt,\n",
    "    engine\n",
    ")\n",
    "best_place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Requete TOP 20 des hostels sur la TOP 5\n",
    "df_top_20 = pd.DataFrame()\n",
    "for x in range(0,5):\n",
    "    stmt = text(\"select hostel.name,hostel.city,hostel.score,hostel.latitude,hostel.longitude from hostel\"\n",
    "                \" inner join propa_precipitation_per_city on hostel.city=propa_precipitation_per_city.city \"\n",
    "                \"where propa_precipitation_per_city.city in \"\n",
    "                \"(select * from (select city \"\n",
    "                \"from propa_precipitation_per_city \"\n",
    "                \"order by total_prediction_rain_per_city ASC \"\n",
    "                f\"offset {x}) AS offset_table LIMIT 1)\"\n",
    "                \"order by hostel.city, hostel.score desc\"\n",
    "                \" limit 20;\")\n",
    "    data_booking_from_sql = pd.read_sql(\n",
    "    stmt,\n",
    "    engine)\n",
    "    df_top_20=df_top_20.append(data_booking_from_sql)\n",
    "\n",
    "\n",
    "df_top_20[\"score\"] = [float(str(i).replace(\",\", \".\")) for i in df_top_20[\"score\"]]\n",
    "df_top_20['latitude']=df_top_20['latitude'].astype(float)\n",
    "df_top_20['longitude']=df_top_20['longitude'].astype(float)\n",
    "df_top_20=df_top_20.reset_index()\n",
    "del df_top_20['index']\n",
    "df_top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pio.renderers.default = \"svg\"\n",
    "px.set_mapbox_access_token('pk.eyJ1IjoiZXRpZW5uZTk3MiIsImEiOiJja3VwaXFrZXAxZTZuMzB0aHZ2Y2d1enNzIn0.tJypjwl5fr8Qo7uqSJjwoA')\n",
    "fig = px.scatter_mapbox(best_place, lat=\"latitude\", lon=\"longitude\",  hover_data=['city'],   color=\"total_prediction_rain_per_city\",color_continuous_scale=px.colors.sequential.Bluered, size_max=15, zoom=5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pio.renderers.default = \"svg\"\n",
    "px.set_mapbox_access_token('pk.eyJ1IjoiZXRpZW5uZTk3MiIsImEiOiJja3VwaXFrZXAxZTZuMzB0aHZ2Y2d1enNzIn0.tJypjwl5fr8Qo7uqSJjwoA')\n",
    "fig = px.scatter_mapbox(df_top_20, lat=\"latitude\", lon=\"longitude\",  hover_data=['name','city'],   color=\"score\",size=\"score\",color_continuous_scale=px.colors.sequential.solar, size_max=15, zoom=5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e23285782446c6864c996c28c3fd325702bb6068d9fb9865cc2d1c7e1d13946"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
