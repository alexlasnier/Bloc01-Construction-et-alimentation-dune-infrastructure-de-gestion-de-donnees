{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è This project is mandatory for certification bloc #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kayak](https://seekvectorlogo.com/wp-content/uploads/2018/01/kayak-vector-logo.png)\n",
    "\n",
    "# Plan your trip with Kayak "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Company's description üìá\n",
    "\n",
    "<a href='https://www.kayak.com' target='_blank'>Kayak</a> is a travel search engine that helps user plan their next trip at the best price.\n",
    "\n",
    "The company was founded in 2004 by Steve Hafner & Paul M. English. After a few rounds of fundraising, Kayak was acquired by <a href='https://www.bookingholdings.com/' target='_blank'>Booking Holdings</a> which now holds: \n",
    "\n",
    "* <a href='https://booking.com/' target='_blank'>Booking.com</a>\n",
    "* <a href='https://kayak.com/' target='_blank'>Kayak</a>\n",
    "* <a href='https://www.priceline.com/' target='_blank'>Priceline</a>\n",
    "* <a href='https://www.agoda.com/' target='_blank'>Agoda</a>\n",
    "* <a href='https://Rentalcars.com/' target='_blank'>RentalCars</a>\n",
    "* <a href='https://www.opentable.com/' target='_blank'>OpenTable</a>\n",
    "\n",
    "With over \\$300 million revenue a year, Kayak operates in almost all countries and all languages to help their users book travels accros the globe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project üöß\n",
    "\n",
    "The marketing team needs help on a new project. After doing some user research, the team discovered that **70% of their users who are planning a trip would like to have more information about the destination they are going to**. \n",
    "\n",
    "In addition, user research shows that **people tend to be defiant about the information they are reading if they don't know the brand** which produced the content. \n",
    "\n",
    "Therefore, Kayak Marketing Team would like to create an application that will recommend where people should plan their next holidays. The application should be based on real data about:\n",
    "\n",
    "* Weather \n",
    "* Hotels in the area \n",
    "\n",
    "The application should then be able to recommend the best destinations and hotels based on the above variables at any given time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals üéØ\n",
    "\n",
    "As the project has just started, your team doesn't have any data that can be used to create this application. Therefore, your job will be to: \n",
    "\n",
    "* Scrape data from destinations \n",
    "* Get weather data from each destination \n",
    "* Get hotels' info about each destination\n",
    "* Store all the information above in a data lake\n",
    "* Extract, transform and load cleaned data from your datalake to a data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of this project üñºÔ∏è\n",
    "\n",
    "Marketing team wants to focus first on the best cities to travel to in France. According <a href='https://one-week-in.com/35-cities-to-visit-in-france/' target='_blank'>One Week In.com</a> here are the top-35 cities to visit in France: \n",
    "\n",
    "```python \n",
    "['Mont Saint Michel',\n",
    "'St Malo',\n",
    "'Bayeux',\n",
    "'Le Havre',\n",
    "'Rouen',\n",
    "'Paris',\n",
    "'Amiens',\n",
    "'Lille',\n",
    "'Strasbourg',\n",
    "'Chateau du Haut Koenigsbourg',\n",
    "'Colmar',\n",
    "'Eguisheim',\n",
    "'Besancon',\n",
    "'Dijon',\n",
    "'Annecy',\n",
    "'Grenoble',\n",
    "'Lyon',\n",
    "'Gorges du Verdon',\n",
    "'Bormes les Mimosas',\n",
    "'Cassis',\n",
    "'Marseille',\n",
    "'Aix en Provence',\n",
    "'Avignon',\n",
    "'Uzes',\n",
    "'Nimes',\n",
    "'Aigues Mortes',\n",
    "'Saintes Maries de la mer',\n",
    "'Collioure',\n",
    "'Carcassonne',\n",
    "'Ariege',\n",
    "'Toulouse',\n",
    "'Montauban',\n",
    "'Biarritz',\n",
    "'Bayonne',\n",
    "'La Rochelle']\n",
    "```\n",
    "\n",
    "Your team should focus **only on the above cities for your project**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers ü¶Æ\n",
    "\n",
    "To help you achieve this project, here are a few tips that should help you\n",
    "\n",
    "### Get weather data with an API \n",
    "\n",
    "*   Use https://nominatim.org/ to get the gps coordinates of all the cities (no subscription required) Documentation : https://nominatim.org/release-docs/develop/api/Search/\n",
    "\n",
    "*   Use https://openweathermap.org/appid (you have to subscribe to get a free apikey) and https://openweathermap.org/api/one-call-api to get some information about the weather for the 35 cities and put it in a DataFrame\n",
    "\n",
    "*   Determine the list of cities where the weather will be the nicest within the next 7 days For example, you can use the values of daily.pop and daily.rain to compute the expected volume of rain within the next 7 days... But it's only an example, actually you can have different opinions on a what a nice weather would be like üòé Maybe the most important criterion for you is the temperature or humidity, so feel free to change the rules !\n",
    "\n",
    "*   Save all the results in a `.csv` file, you will use it later üòâ You can save all the informations that seem important to you ! Don't forget to save the name of the cities, and also to create a column containing a unique identifier (id) of each city (this is important for what's next in the project)\n",
    "\n",
    "*   Use plotly to display the best destinations on a map\n",
    "\n",
    "### Scrape Booking.com \n",
    "\n",
    "Since BookingHoldings doesn't have aggregated databases, it will be much faster to scrape data directly from booking.com \n",
    "\n",
    "You can scrap as many information asyou want, but we suggest that you get at least:\n",
    "\n",
    "*   hotel name,\n",
    "*   Url to its booking.com page,\n",
    "*   Its coordinates: latitude and longitude\n",
    "*   Score given by the website users\n",
    "*   Text description of the hotel\n",
    "\n",
    "\n",
    "### Create your data lake using S3 \n",
    "\n",
    "Once you managed to build your dataset, you should store into S3 as a csv file. \n",
    "\n",
    "### ETL \n",
    "\n",
    "Once you uploaded your data onto S3, it will be better for the next data analysis team to extract clean data directly from a Data Warehouse. Therefore, create a SQL Database using AWS RDS, extract your data from S3 and store it in your newly created DB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable üì¨\n",
    "\n",
    "To complete this project, your team should deliver:\n",
    "\n",
    "* A `.csv` file in an S3 bucket containing enriched information about weather and hotels for each french city\n",
    "\n",
    "* A SQL Database where we should be able to get the same cleaned data from S3 \n",
    "\n",
    "* Two maps where you should have a Top-5 destinations and a Top-20 hotels in the area. You can use plotly or any other library to do so. It should look something like this: \n",
    "\n",
    "![Map](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/Kayak_best_destination_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weather data with an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of cities\n",
    "\n",
    "cities = [\n",
    "    \"Mont Saint Michel\",\n",
    "    \"St Malo\",\n",
    "    \"Bayeux\",\n",
    "    \"Le Havre\",\n",
    "    \"Rouen\",\n",
    "    \"Paris\",\n",
    "    \"Amiens\",\n",
    "    \"Lille\",\n",
    "    \"Strasbourg\",\n",
    "    \"Chateau du Haut Koenigsbourg\",\n",
    "    \"Colmar\",\n",
    "    \"Eguisheim\",\n",
    "    \"Besancon\",\n",
    "    \"Dijon\",\n",
    "    \"Annecy\",\n",
    "    \"Grenoble\",\n",
    "    \"Lyon\",\n",
    "    \"Gorges du Verdon\",\n",
    "    \"Bormes les Mimosas\",\n",
    "    \"Cassis\",\n",
    "    \"Marseille\",\n",
    "    \"Aix en Provence\",\n",
    "    \"Avignon\",\n",
    "    \"Uzes\",\n",
    "    \"Nimes\",\n",
    "    \"Aigues Mortes\",\n",
    "    \"Saintes Maries de la mer\",\n",
    "    \"Collioure\",\n",
    "    \"Carcassonne\",\n",
    "    \"Ariege\",\n",
    "    \"Toulouse\",\n",
    "    \"Montauban\",\n",
    "    \"Biarritz\",\n",
    "    \"Bayonne\",\n",
    "    \"La Rochelle\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a dataframe from the list of cities\n",
    "df_cities = pd.DataFrame(data=cities, columns=[\"city\"])\n",
    "df_cities = df_cities.reset_index()\n",
    "df_cities = df_cities.rename(columns={\"index\": \"id\"})\n",
    "df_cities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominatim API : get the gps coordinates of all the cities => https://nominatim.org/release-docs/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a city with blank space in name to see if formatting needed:\n",
    "import requests\n",
    "\n",
    "params = {\"city\": \"La Rochelle\", \"country\": \"France\", \"format\": \"json\"}\n",
    "\n",
    "r = requests.get(\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know 'Chateau du Haut Koenigsbourg', check if Nominatim knows it\n",
    "import requests\n",
    "\n",
    "params = {\"city\": \"Chateau du Haut Koenigsbourg\", \"country\": \"France\", \"format\": \"json\"}\n",
    "\n",
    "r = requests.get(\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Gorges du Verdon' isn't a city but a canyon, check if Nominatim works with or not\n",
    "import requests\n",
    "\n",
    "params = {\"city\": \"Gorges du Verdon\", \"country\": \"France\", \"format\": \"json\"}\n",
    "\n",
    "r = requests.get(\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "\n",
    "r\n",
    "\n",
    "# Doesnt' works... Google search : 'Castellane' will be use for Gorges du Verdon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ariege is a department and not a city, its prefecture is 'Foix'. We'll use this city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of df_cities to store the coordinates from Nominatim API\n",
    "\n",
    "df_gps = df_cities.copy()\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "\n",
    "for i in cities:\n",
    "    print(f\"Request for city: {i}\")\n",
    "    params = {\"city\": i, \"country\": \"France\", \"format\": \"json\"}\n",
    "    # No 'Gorges du Verdon' city, replacing by 'Castellane'\n",
    "    if i == \"Gorges du Verdon\":\n",
    "        i = \"Castellane\"\n",
    "        r = requests.get(\n",
    "            f\"https://nominatim.openstreetmap.org/search?city={i}&country=France&format=json\"\n",
    "        ).json()\n",
    "        lat_list.append(r[0][\"lat\"])\n",
    "        lon_list.append(r[0][\"lon\"])\n",
    "    # 'Ariege' not a city, using the prefecture instead -> 'Foix'\n",
    "    elif i == \"Ariege\":\n",
    "        i = \"Foix\"\n",
    "        r = requests.get(\n",
    "            f\"https://nominatim.openstreetmap.org/search?city={i}&country=France&format=json\"\n",
    "        ).json()\n",
    "        lat_list.append(r[0][\"lat\"])\n",
    "        lon_list.append(r[0][\"lon\"])\n",
    "    else:\n",
    "        r = requests.get(f\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "        lat_list.append(r[0][\"lat\"])\n",
    "        lon_list.append(r[0][\"lon\"])\n",
    "\n",
    "# Adding the coordinates to the dataframe\n",
    "df_gps[\"lat\"] = lat_list\n",
    "df_gps[\"lon\"] = lon_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_gps.head())\n",
    "\n",
    "print()\n",
    "\n",
    "print(df_gps.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenWeather : get weather of the week => https://openweathermap.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try at St Malo\n",
    "parameters = {\n",
    "    \"lat\": 48.649518,\n",
    "    \"lon\": -2.0260409,\n",
    "    \"exclude\": \"current,minutely,hourly\",\n",
    "    \"units\": \"metric\",\n",
    "    \"appid\": \"aa423e6694bf72625fe1fe31544949dc\",\n",
    "    \"lang\": \"fr\",\n",
    "}\n",
    "\n",
    "r = requests.get(\n",
    "    \"https://api.openweathermap.org/data/2.5/onecall\", params=parameters\n",
    ").json()\n",
    "\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather in 3 days at St Malo\n",
    "r[\"daily\"][3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day3 = r[\"daily\"][3]  # Weather in 3 days\n",
    "\n",
    "# desciption of the main weather in 3 days at St Malo\n",
    "day3[\"weather\"][0][\"main\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_weather = df_gps.copy(deep=True)\n",
    "my_api_key = \"aa423e6694bf72625fe1fe31544949dc\"\n",
    "\n",
    "temperatures_list = []\n",
    "rain_list = []\n",
    "weather_list = []\n",
    "\n",
    "days = list(range(1, 8))\n",
    "\n",
    "for i in df_weather.itertuples():\n",
    "    lat = i.lat\n",
    "    lon = i.lon\n",
    "\n",
    "    parameters = {\n",
    "        \"lat\": {lat},\n",
    "        \"lon\": {lon},\n",
    "        \"exclude\": \"current,minutely,hourly\",\n",
    "        \"units\": \"metric\",\n",
    "        \"appid\": \"aa423e6694bf72625fe1fe31544949dc\",\n",
    "        \"lang\": \"fr\",\n",
    "    }\n",
    "\n",
    "    r = requests.get(\n",
    "        f\"https://api.openweathermap.org/data/2.5/onecall?\", parameters\n",
    "    ).json()\n",
    "    forecast_7days = r[\"daily\"][\n",
    "        1:\n",
    "    ]  # Getting the weather data for the next 7 days, first item is the current weather, which we don't want here\n",
    "    temperatures = [int(d[\"feels_like\"][\"day\"]) for d in forecast_7days]\n",
    "    rain = [int(d[\"pop\"] * 100) for d in forecast_7days]\n",
    "    weather = [str(d[\"weather\"][0][\"main\"]) for d in forecast_7days]\n",
    "    temperatures_list.append(temperatures)\n",
    "    rain_list.append(rain)\n",
    "    weather_list.append(weather)\n",
    "\n",
    "df_weather[\"jour_+x\"] = [days for _ in range(len(df_weather))]\n",
    "df_weather[\"temperature_ressentie\"] = temperatures_list\n",
    "df_weather[\"probabilite_de_pluie\"] = rain_list\n",
    "df_weather[\"meteo_principale\"] = weather_list\n",
    "df_weather[\"score\"] = df_weather.apply(\n",
    "    lambda x: ((np.mean(x[\"temperature_ressentie\"])) * 2)\n",
    "    + np.mean(x[\"probabilite_de_pluie\"]),\n",
    "    axis=1,\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "print(df_weather.head())\n",
    "\n",
    "print()\n",
    "\n",
    "print(df_weather.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather[[\"lat\", \"lon\"]] = df_weather[[\"lat\", \"lon\"]].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_weather.apply(pd.Series.explode)  # To obtain a line per day and per city\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print()\n",
    "\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"jour_+x\", \"temperature_ressentie\", \"probabilite_de_pluie\"]] = df[\n",
    "    [\"jour_+x\", \"temperature_ressentie\", \"probabilite_de_pluie\"]\n",
    "].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    df,\n",
    "    lat=\"lat\",\n",
    "    lon=\"lon\",\n",
    "    hover_name=\"city\",\n",
    "    zoom=4,\n",
    "    hover_data=[\"meteo_principale\", \"probabilite_de_pluie\", \"temperature_ressentie\"],\n",
    "    color=\"temperature_ressentie\",\n",
    "    color_continuous_scale=\"thermal\",\n",
    "    mapbox_style=\"carto-positron\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Booking.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get hotels, their URLs, their notes and their description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class BookingSpider(scrapy.Spider):\n",
    "    # Name of my spider\n",
    "    name = \"booking_spider\"\n",
    "    cities = df_weather.city\n",
    "    # Url to start my spider from\n",
    "    start_urls = [\n",
    "        \"https://www.booking.com/index.fr.html\",\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting my spider\n",
    "    def parse(self, response):\n",
    "        for x in cities:\n",
    "            yield scrapy.FormRequest.from_response(\n",
    "                response, formdata={\"ss\": x}, callback=self.after_search\n",
    "            )\n",
    "\n",
    "    def after_search(self, response):\n",
    "        cities = response.url.split(\"ss=\")[-1].split(\"&\")[0]\n",
    "        booking = response.css(\".d4924c9e74\")\n",
    "\n",
    "        for data in booking:\n",
    "\n",
    "            yield {\n",
    "                \"ville\": cities,\n",
    "                \"hotels\": data.css(\"a div.fcab3ed991.a23c043802::text\").getall(),\n",
    "                \"liens\": data.css(\"h3.a4225678b2 a::attr(href)\").getall(),\n",
    "                #'note': data.css('div.b5cd09854e.d10a6220b4::text').getall(),\n",
    "                #'description' : data.css ('div.d8eab2cf7f::text').getall()\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            next_page = response.css(\"a.paging-next\").attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info(\"No next page. Terminating crawling process.\")\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"hotels.json\"\n",
    "\n",
    "# If file already exists, delete before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir():\n",
    "    os.remove(filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "        \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.112 Safari/535.1\",\n",
    "        \"LOG_LEVEL\": logging.INFO,\n",
    "        \"FEEDS\": {\n",
    "            filename: {\"format\": \"json\"},\n",
    "        },\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(BookingSpider)\n",
    "process.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get hotels' coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ville</th>\n",
       "      <th>hotels</th>\n",
       "      <th>liens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mont+Saint+Michel</td>\n",
       "      <td>[H√¥tel Vert, Les Terrasses Poulard, Mercure Mo...</td>\n",
       "      <td>[https://www.booking.com/hotel/fr/vert.fr.html...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>St+Malo</td>\n",
       "      <td>[Ambassadeurs Logis Hotel, Residence Reine Mar...</td>\n",
       "      <td>[https://www.booking.com/hotel/fr/ambassadeurs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bayeux</td>\n",
       "      <td>[ibis budget Bayeux, Domaine de Bayeux, Ch√¢tea...</td>\n",
       "      <td>[https://www.booking.com/hotel/fr/etap-bayeux....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Le+Havre</td>\n",
       "      <td>[H√¥tel Le Marignan - Le Havre Centre Gare, Hol...</td>\n",
       "      <td>[https://www.booking.com/hotel/fr/la-baraka.fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rouen</td>\n",
       "      <td>[Radisson Blu Hotel, Rouen Centre, Mercure Rou...</td>\n",
       "      <td>[https://www.booking.com/hotel/fr/radisson-blu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ville                                             hotels  \\\n",
       "0  Mont+Saint+Michel  [H√¥tel Vert, Les Terrasses Poulard, Mercure Mo...   \n",
       "1            St+Malo  [Ambassadeurs Logis Hotel, Residence Reine Mar...   \n",
       "2             Bayeux  [ibis budget Bayeux, Domaine de Bayeux, Ch√¢tea...   \n",
       "3           Le+Havre  [H√¥tel Le Marignan - Le Havre Centre Gare, Hol...   \n",
       "4              Rouen  [Radisson Blu Hotel, Rouen Centre, Mercure Rou...   \n",
       "\n",
       "                                               liens  \n",
       "0  [https://www.booking.com/hotel/fr/vert.fr.html...  \n",
       "1  [https://www.booking.com/hotel/fr/ambassadeurs...  \n",
       "2  [https://www.booking.com/hotel/fr/etap-bayeux....  \n",
       "3  [https://www.booking.com/hotel/fr/la-baraka.fr...  \n",
       "4  [https://www.booking.com/hotel/fr/radisson-blu...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the json file\n",
    "import pandas as pd\n",
    "\n",
    "df_scraped = pd.read_json(\"hotels.json\")\n",
    "df_scraped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 20:19:27 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-24 20:19:27 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  5 2022, 01:53:17) - [Clang 12.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2022-05-24 20:19:27 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 5.1; rv:11.0) Gecko Firefox/11.0'}\n",
      "2022-05-24 20:19:27 [scrapy.extensions.telnet] INFO: Telnet Password: 8c36be2119269e21\n",
      "2022-05-24 20:19:27 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-05-24 20:19:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-24 20:19:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-24 20:19:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-24 20:19:28 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-24 20:19:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-24 20:19:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-24 20:20:28 [scrapy.extensions.logstats] INFO: Crawled 35 pages (at 35 pages/min), scraped 35 items (at 35 items/min)\n",
      "2022-05-24 20:21:28 [scrapy.extensions.logstats] INFO: Crawled 82 pages (at 47 pages/min), scraped 82 items (at 47 items/min)\n",
      "2022-05-24 20:22:28 [scrapy.extensions.logstats] INFO: Crawled 127 pages (at 45 pages/min), scraped 126 items (at 44 items/min)\n",
      "2022-05-24 20:23:28 [scrapy.extensions.logstats] INFO: Crawled 171 pages (at 44 pages/min), scraped 171 items (at 45 items/min)\n",
      "2022-05-24 20:24:28 [scrapy.extensions.logstats] INFO: Crawled 218 pages (at 47 pages/min), scraped 218 items (at 47 items/min)\n",
      "2022-05-24 20:25:28 [scrapy.extensions.logstats] INFO: Crawled 261 pages (at 43 pages/min), scraped 261 items (at 43 items/min)\n",
      "2022-05-24 20:26:28 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 41 pages/min), scraped 302 items (at 41 items/min)\n",
      "2022-05-24 20:27:28 [scrapy.extensions.logstats] INFO: Crawled 349 pages (at 47 pages/min), scraped 349 items (at 47 items/min)\n",
      "2022-05-24 20:28:28 [scrapy.extensions.logstats] INFO: Crawled 398 pages (at 49 pages/min), scraped 398 items (at 49 items/min)\n",
      "2022-05-24 20:29:28 [scrapy.extensions.logstats] INFO: Crawled 447 pages (at 49 pages/min), scraped 446 items (at 48 items/min)\n",
      "2022-05-24 20:30:28 [scrapy.extensions.logstats] INFO: Crawled 493 pages (at 46 pages/min), scraped 493 items (at 47 items/min)\n",
      "2022-05-24 20:31:28 [scrapy.extensions.logstats] INFO: Crawled 543 pages (at 50 pages/min), scraped 542 items (at 49 items/min)\n",
      "2022-05-24 20:32:28 [scrapy.extensions.logstats] INFO: Crawled 590 pages (at 47 pages/min), scraped 590 items (at 48 items/min)\n",
      "2022-05-24 20:33:28 [scrapy.extensions.logstats] INFO: Crawled 637 pages (at 47 pages/min), scraped 637 items (at 47 items/min)\n",
      "2022-05-24 20:34:28 [scrapy.extensions.logstats] INFO: Crawled 682 pages (at 45 pages/min), scraped 682 items (at 45 items/min)\n",
      "2022-05-24 20:35:28 [scrapy.extensions.logstats] INFO: Crawled 729 pages (at 47 pages/min), scraped 729 items (at 47 items/min)\n",
      "2022-05-24 20:36:28 [scrapy.extensions.logstats] INFO: Crawled 775 pages (at 46 pages/min), scraped 775 items (at 46 items/min)\n",
      "2022-05-24 20:37:28 [scrapy.extensions.logstats] INFO: Crawled 822 pages (at 47 pages/min), scraped 822 items (at 47 items/min)\n",
      "2022-05-24 20:38:28 [scrapy.extensions.logstats] INFO: Crawled 870 pages (at 48 pages/min), scraped 870 items (at 48 items/min)\n",
      "2022-05-24 20:38:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-24 20:38:35 [scrapy.extensions.feedexport] INFO: Stored json feed (875 items) in: hotels_coords.json\n",
      "2022-05-24 20:38:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1004662,\n",
      " 'downloader/request_count': 875,\n",
      " 'downloader/request_method_count/GET': 875,\n",
      " 'downloader/response_bytes': 191529863,\n",
      " 'downloader/response_count': 875,\n",
      " 'downloader/response_status_count/200': 875,\n",
      " 'elapsed_time_seconds': 1146.917079,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 24, 18, 38, 35, 72909),\n",
      " 'httpcompression/response_bytes': 1143578127,\n",
      " 'httpcompression/response_count': 875,\n",
      " 'item_scraped_count': 875,\n",
      " 'log_count/INFO': 30,\n",
      " 'memusage/max': 789626880,\n",
      " 'memusage/startup': 162541568,\n",
      " 'response_received_count': 875,\n",
      " 'scheduler/dequeued': 875,\n",
      " 'scheduler/dequeued/memory': 875,\n",
      " 'scheduler/enqueued': 875,\n",
      " 'scheduler/enqueued/memory': 875,\n",
      " 'start_time': datetime.datetime(2022, 5, 24, 18, 19, 28, 155830)}\n",
      "2022-05-24 20:38:35 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "class CoordsSpider(scrapy.Spider):\n",
    "    # Name of my spider\n",
    "    name = 'coords_spider'\n",
    "    cities = df_scraped.ville\n",
    "\n",
    "    # Url to start my spider from\n",
    "    start_urls = []\n",
    "    for i in range(0,35):\n",
    "        start_urls += df_scraped['liens'][i]\n",
    "\n",
    "    # Callback function that will be called when starting my spider\n",
    "    def parse(self, response):\n",
    "\n",
    "        yield {\n",
    "            #'ville': cities,\n",
    "            'lat' : (response.css('p.address.address_clean a::attr(data-atlas-latlng)').get().split(\",\")[0]),\n",
    "            'lon' : (response.css('p.address.address_clean a::attr(data-atlas-latlng)').get().split(\",\")[1])\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"hotels_coords.json\"\n",
    "\n",
    "# If file already exists, delete before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir():\n",
    "    os.remove(filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "        \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 5.1; rv:11.0) Gecko Firefox/11.0\",\n",
    "        \"LOG_LEVEL\": logging.INFO,\n",
    "        \"FEEDS\": {\n",
    "            filename: {\"format\": \"json\"},\n",
    "        },\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(CoordsSpider)\n",
    "process.start()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.614700</td>\n",
       "      <td>-1.509617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.635349</td>\n",
       "      <td>-1.510379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.614247</td>\n",
       "      <td>-1.510545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.617587</td>\n",
       "      <td>-1.510396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.616882</td>\n",
       "      <td>-1.510918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>45.746023</td>\n",
       "      <td>4.823894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>45.737051</td>\n",
       "      <td>4.837399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>45.757690</td>\n",
       "      <td>4.858226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>45.749297</td>\n",
       "      <td>4.829564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>45.749883</td>\n",
       "      <td>4.829924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>875 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lat       lon\n",
       "0    48.614700 -1.509617\n",
       "1    48.635349 -1.510379\n",
       "2    48.614247 -1.510545\n",
       "3    48.617587 -1.510396\n",
       "4    48.616882 -1.510918\n",
       "..         ...       ...\n",
       "870  45.746023  4.823894\n",
       "871  45.737051  4.837399\n",
       "872  45.757690  4.858226\n",
       "873  45.749297  4.829564\n",
       "874  45.749883  4.829924\n",
       "\n",
       "[875 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords = pd.read_json(\"hotels_coords.json\")\n",
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your data lake using S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e23285782446c6864c996c28c3fd325702bb6068d9fb9865cc2d1c7e1d13946"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
