{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è This project is mandatory for certification bloc #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kayak](https://seekvectorlogo.com/wp-content/uploads/2018/01/kayak-vector-logo.png)\n",
    "\n",
    "# Plan your trip with Kayak "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Company's description üìá\n",
    "\n",
    "<a href='https://www.kayak.com' target='_blank'>Kayak</a> is a travel search engine that helps user plan their next trip at the best price.\n",
    "\n",
    "The company was founded in 2004 by Steve Hafner & Paul M. English. After a few rounds of fundraising, Kayak was acquired by <a href='https://www.bookingholdings.com/' target='_blank'>Booking Holdings</a> which now holds: \n",
    "\n",
    "* <a href='https://booking.com/' target='_blank'>Booking.com</a>\n",
    "* <a href='https://kayak.com/' target='_blank'>Kayak</a>\n",
    "* <a href='https://www.priceline.com/' target='_blank'>Priceline</a>\n",
    "* <a href='https://www.agoda.com/' target='_blank'>Agoda</a>\n",
    "* <a href='https://Rentalcars.com/' target='_blank'>RentalCars</a>\n",
    "* <a href='https://www.opentable.com/' target='_blank'>OpenTable</a>\n",
    "\n",
    "With over \\$300 million revenue a year, Kayak operates in almost all countries and all languages to help their users book travels accros the globe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project üöß\n",
    "\n",
    "The marketing team needs help on a new project. After doing some user research, the team discovered that **70% of their users who are planning a trip would like to have more information about the destination they are going to**. \n",
    "\n",
    "In addition, user research shows that **people tend to be defiant about the information they are reading if they don't know the brand** which produced the content. \n",
    "\n",
    "Therefore, Kayak Marketing Team would like to create an application that will recommend where people should plan their next holidays. The application should be based on real data about:\n",
    "\n",
    "* Weather \n",
    "* Hotels in the area \n",
    "\n",
    "The application should then be able to recommend the best destinations and hotels based on the above variables at any given time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals üéØ\n",
    "\n",
    "As the project has just started, your team doesn't have any data that can be used to create this application. Therefore, your job will be to: \n",
    "\n",
    "* Scrape data from destinations \n",
    "* Get weather data from each destination \n",
    "* Get hotels' info about each destination\n",
    "* Store all the information above in a data lake\n",
    "* Extract, transform and load cleaned data from your datalake to a data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of this project üñºÔ∏è\n",
    "\n",
    "Marketing team wants to focus first on the best cities to travel to in France. According <a href='https://one-week-in.com/35-cities-to-visit-in-france/' target='_blank'>One Week In.com</a> here are the top-35 cities to visit in France: \n",
    "\n",
    "```python \n",
    "['Mont Saint Michel',\n",
    "'St Malo',\n",
    "'Bayeux',\n",
    "'Le Havre',\n",
    "'Rouen',\n",
    "'Paris',\n",
    "'Amiens',\n",
    "'Lille',\n",
    "'Strasbourg',\n",
    "'Chateau du Haut Koenigsbourg',\n",
    "'Colmar',\n",
    "'Eguisheim',\n",
    "'Besancon',\n",
    "'Dijon',\n",
    "'Annecy',\n",
    "'Grenoble',\n",
    "'Lyon',\n",
    "'Gorges du Verdon',\n",
    "'Bormes les Mimosas',\n",
    "'Cassis',\n",
    "'Marseille',\n",
    "'Aix en Provence',\n",
    "'Avignon',\n",
    "'Uzes',\n",
    "'Nimes',\n",
    "'Aigues Mortes',\n",
    "'Saintes Maries de la mer',\n",
    "'Collioure',\n",
    "'Carcassonne',\n",
    "'Ariege',\n",
    "'Toulouse',\n",
    "'Montauban',\n",
    "'Biarritz',\n",
    "'Bayonne',\n",
    "'La Rochelle']\n",
    "```\n",
    "\n",
    "Your team should focus **only on the above cities for your project**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers ü¶Æ\n",
    "\n",
    "To help you achieve this project, here are a few tips that should help you\n",
    "\n",
    "### Get weather data with an API \n",
    "\n",
    "*   Use https://nominatim.org/ to get the gps coordinates of all the cities (no subscription required) Documentation : https://nominatim.org/release-docs/develop/api/Search/\n",
    "\n",
    "*   Use https://openweathermap.org/appid (you have to subscribe to get a free apikey) and https://openweathermap.org/api/one-call-api to get some information about the weather for the 35 cities and put it in a DataFrame\n",
    "\n",
    "*   Determine the list of cities where the weather will be the nicest within the next 7 days For example, you can use the values of daily.pop and daily.rain to compute the expected volume of rain within the next 7 days... But it's only an example, actually you can have different opinions on a what a nice weather would be like üòé Maybe the most important criterion for you is the temperature or humidity, so feel free to change the rules !\n",
    "\n",
    "*   Save all the results in a `.csv` file, you will use it later üòâ You can save all the informations that seem important to you ! Don't forget to save the name of the cities, and also to create a column containing a unique identifier (id) of each city (this is important for what's next in the project)\n",
    "\n",
    "*   Use plotly to display the best destinations on a map\n",
    "\n",
    "### Scrape Booking.com \n",
    "\n",
    "Since BookingHoldings doesn't have aggregated databases, it will be much faster to scrape data directly from booking.com \n",
    "\n",
    "You can scrap as many information asyou want, but we suggest that you get at least:\n",
    "\n",
    "*   hotel name,\n",
    "*   Url to its booking.com page,\n",
    "*   Its coordinates: latitude and longitude\n",
    "*   Score given by the website users\n",
    "*   Text description of the hotel\n",
    "\n",
    "\n",
    "### Create your data lake using S3 \n",
    "\n",
    "Once you managed to build your dataset, you should store into S3 as a csv file. \n",
    "\n",
    "### ETL \n",
    "\n",
    "Once you uploaded your data onto S3, it will be better for the next data analysis team to extract clean data directly from a Data Warehouse. Therefore, create a SQL Database using AWS RDS, extract your data from S3 and store it in your newly created DB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable üì¨\n",
    "\n",
    "To complete this project, your team should deliver:\n",
    "\n",
    "* A `.csv` file in an S3 bucket containing enriched information about weather and hotels for each french city\n",
    "\n",
    "* A SQL Database where we should be able to get the same cleaned data from S3 \n",
    "\n",
    "* Two maps where you should have a Top-5 destinations and a Top-20 hotels in the area. You can use plotly or any other library to do so. It should look something like this: \n",
    "\n",
    "![Map](https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/Kayak_best_destination_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs:\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import logging\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weather data with an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of cities\n",
    "\n",
    "cities = [\n",
    "    \"Mont Saint Michel\",\n",
    "    \"St Malo\",\n",
    "    \"Bayeux\",\n",
    "    \"Le Havre\",\n",
    "    \"Rouen\",\n",
    "    \"Paris\",\n",
    "    \"Amiens\",\n",
    "    \"Lille\",\n",
    "    \"Strasbourg\",\n",
    "    \"Chateau du Haut Koenigsbourg\",\n",
    "    \"Colmar\",\n",
    "    \"Eguisheim\",\n",
    "    \"Besancon\",\n",
    "    \"Dijon\",\n",
    "    \"Annecy\",\n",
    "    \"Grenoble\",\n",
    "    \"Lyon\",\n",
    "    \"Gorges du Verdon\",\n",
    "    \"Bormes les Mimosas\",\n",
    "    \"Cassis\",\n",
    "    \"Marseille\",\n",
    "    \"Aix en Provence\",\n",
    "    \"Avignon\",\n",
    "    \"Uzes\",\n",
    "    \"Nimes\",\n",
    "    \"Aigues Mortes\",\n",
    "    \"Saintes Maries de la mer\",\n",
    "    \"Collioure\",\n",
    "    \"Carcassonne\",\n",
    "    \"Ariege\",\n",
    "    \"Toulouse\",\n",
    "    \"Montauban\",\n",
    "    \"Biarritz\",\n",
    "    \"Bayonne\",\n",
    "    \"La Rochelle\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the list of cities\n",
    "df_cities = pd.DataFrame(data=cities, columns=[\"city\"])\n",
    "df_cities = df_cities.reset_index()\n",
    "df_cities = df_cities.rename(columns={\"index\": \"id\"})\n",
    "df_cities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominatim API : get the gps coordinates of all the cities => https://nominatim.org/release-docs/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a city with blank space in name to see if formatting needed:\n",
    "\n",
    "params = {\"city\": \"La Rochelle\", \"country\": \"France\", \"format\": \"json\"}\n",
    "\n",
    "r = requests.get(\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know 'Chateau du Haut Koenigsbourg', check if Nominatim knows it\n",
    "\n",
    "params = {\"city\": \"Chateau du Haut Koenigsbourg\", \"country\": \"France\", \"format\": \"json\"}\n",
    "\n",
    "r = requests.get(\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Gorges du Verdon' isn't a city but a canyon, check if Nominatim works with or not\n",
    "\n",
    "params = {\"city\": \"Gorges du Verdon\", \"country\": \"France\", \"format\": \"json\"}\n",
    "\n",
    "r = requests.get(\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "\n",
    "r\n",
    "\n",
    "# Doesnt' works... Google search : 'Castellane' will be use for Gorges du Verdon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ariege is a department and not a city, its prefecture is 'Foix'. We'll use this city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of df_cities to store the coordinates from Nominatim API\n",
    "\n",
    "df_gps = df_cities.copy()\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "\n",
    "for i in cities:\n",
    "    print(f\"Request for city: {i}\")\n",
    "    params = {\"city\": i, \"country\": \"France\", \"format\": \"json\"}\n",
    "    # No 'Gorges du Verdon' city, replacing by 'Castellane'\n",
    "    if i == \"Gorges du Verdon\":\n",
    "        i = \"Castellane\"\n",
    "        r = requests.get(\n",
    "            f\"https://nominatim.openstreetmap.org/search?city={i}&country=France&format=json\"\n",
    "        ).json()\n",
    "        lat_list.append(r[0][\"lat\"])\n",
    "        lon_list.append(r[0][\"lon\"])\n",
    "    # 'Ariege' not a city, using the prefecture instead -> 'Foix'\n",
    "    elif i == \"Ariege\":\n",
    "        i = \"Foix\"\n",
    "        r = requests.get(\n",
    "            f\"https://nominatim.openstreetmap.org/search?city={i}&country=France&format=json\"\n",
    "        ).json()\n",
    "        lat_list.append(r[0][\"lat\"])\n",
    "        lon_list.append(r[0][\"lon\"])\n",
    "    else:\n",
    "        r = requests.get(f\"https://nominatim.openstreetmap.org/search?\", params).json()\n",
    "        lat_list.append(r[0][\"lat\"])\n",
    "        lon_list.append(r[0][\"lon\"])\n",
    "\n",
    "# Adding the coordinates to the dataframe\n",
    "df_gps[\"lat\"] = lat_list\n",
    "df_gps[\"lon\"] = lon_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenWeather : get weather of the week => https://openweathermap.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try at St Malo\n",
    "parameters = {\n",
    "    \"lat\": 48.649518,\n",
    "    \"lon\": -2.0260409,\n",
    "    \"exclude\": \"current,minutely,hourly\",\n",
    "    \"units\": \"metric\",\n",
    "    \"appid\": \"aa423e6694bf72625fe1fe31544949dc\",\n",
    "    \"lang\": \"fr\",\n",
    "}\n",
    "\n",
    "r = requests.get(\n",
    "    \"https://api.openweathermap.org/data/2.5/onecall\", params=parameters\n",
    ").json()\n",
    "\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather in 3 days at St Malo\n",
    "r[\"daily\"][3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day3 = r[\"daily\"][3]  # Weather in 3 days\n",
    "\n",
    "# desciption of the main weather in 3 days at St Malo\n",
    "day3[\"weather\"][0][\"main\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps_weather = df_gps.copy(deep=True)\n",
    "my_api_key = \"aa423e6694bf72625fe1fe31544949dc\"\n",
    "\n",
    "temperatures_list = []\n",
    "rain_list = []\n",
    "weather_list = []\n",
    "\n",
    "days = list(range(1, 8))\n",
    "\n",
    "for i in df_gps_weather.itertuples():\n",
    "    lat = i.lat\n",
    "    lon = i.lon\n",
    "\n",
    "    parameters = {\n",
    "        \"lat\": {lat},\n",
    "        \"lon\": {lon},\n",
    "        \"exclude\": \"current,minutely,hourly\",\n",
    "        \"units\": \"metric\",\n",
    "        \"appid\": \"aa423e6694bf72625fe1fe31544949dc\",\n",
    "        \"lang\": \"fr\",\n",
    "    }\n",
    "\n",
    "    r = requests.get(\n",
    "        f\"https://api.openweathermap.org/data/2.5/onecall?\", parameters\n",
    "    ).json()\n",
    "    forecast_7days = r[\"daily\"][1:]  # Getting the weather data for the next 7 days\n",
    "    temperatures = [int(d[\"feels_like\"][\"day\"]) for d in forecast_7days]\n",
    "    rain = [int(d[\"pop\"] * 100) for d in forecast_7days]\n",
    "    weather = [str(d[\"weather\"][0][\"main\"]) for d in forecast_7days]\n",
    "    temperatures_list.append(temperatures)\n",
    "    rain_list.append(rain)\n",
    "    weather_list.append(weather)\n",
    "\n",
    "df_gps_weather[\"jour_+x\"] = [days for _ in range(len(df_gps_weather))]\n",
    "df_gps_weather[\"temperature_ressentie\"] = temperatures_list\n",
    "df_gps_weather[\"probabilite_de_pluie\"] = rain_list\n",
    "df_gps_weather[\"meteo_principale\"] = weather_list\n",
    "# df_weather['rang'] = df_weather['probabilite_de_pluie'].sort_values()\n",
    "df_gps_weather[\"score\"] = df_gps_weather.apply(\n",
    "    lambda x: ((np.mean(x[\"temperature_ressentie\"])))\n",
    "    - (np.mean(x[\"probabilite_de_pluie\"]) / 10),\n",
    "    axis=1,\n",
    ").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps_weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps_weather.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps_weather[[\"lat\", \"lon\"]] = df_gps_weather[[\"lat\", \"lon\"]].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of cities where the weather will be the nicest within the next 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps_weather.sort_values(by=[\"score\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all the results in a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gps_weather.to_csv(\"df_gps_weather.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly to display the best destinations on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plotly = df_gps_weather.apply(\n",
    "    pd.Series.explode\n",
    ")  # To obtain a line per day and per city\n",
    "\n",
    "df_plotly[[\"jour_+x\", \"temperature_ressentie\", \"probabilite_de_pluie\"]] = df_plotly[\n",
    "    [\"jour_+x\", \"temperature_ressentie\", \"probabilite_de_pluie\"]\n",
    "].astype(int)\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    df_plotly,\n",
    "    lat=\"lat\",\n",
    "    lon=\"lon\",\n",
    "    hover_name=\"city\",\n",
    "    zoom=4,\n",
    "    hover_data=[\"meteo_principale\", \"probabilite_de_pluie\", \"temperature_ressentie\"],\n",
    "    color=\"temperature_ressentie\",\n",
    "    color_continuous_scale=\"thermal\",\n",
    "    mapbox_style=\"carto-positron\",\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Booking.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get hotels and their URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookingSpider(scrapy.Spider):\n",
    "    # Name of my spider\n",
    "    name = \"booking_spider\"\n",
    "    cities = df_gps_weather.city\n",
    "    # Url to start my spider from\n",
    "    start_urls = [\n",
    "        \"https://www.booking.com/index.fr.html\",\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting my spider\n",
    "    def parse(self, response):\n",
    "        for x in cities:\n",
    "            yield scrapy.FormRequest.from_response(\n",
    "                response, formdata={\"ss\": x}, callback=self.after_search\n",
    "            )\n",
    "\n",
    "    def after_search(self, response):\n",
    "        cities = response.url.split(\"ss=\")[-1].split(\"&\")[0]\n",
    "        booking = response.css(\".d4924c9e74\")\n",
    "\n",
    "        for data in booking:\n",
    "\n",
    "            yield {\n",
    "                \"ville\": cities,\n",
    "                \"hotels\": data.css(\"a div.fcab3ed991.a23c043802::text\").getall(),\n",
    "                \"liens\": data.css(\"h3.a4225678b2 a::attr(href)\").getall(),\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            next_page = response.css(\"a.paging-next\").attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info(\"No next page. Terminating crawling process.\")\n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)\n",
    "\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"hotels.json\"\n",
    "\n",
    "# If file already exists, delete before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir():\n",
    "    os.remove(filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "        \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.112 Safari/535.1\",\n",
    "        \"LOG_LEVEL\": logging.INFO,\n",
    "        \"FEEDS\": {\n",
    "            filename: {\"format\": \"json\"},\n",
    "        },\n",
    "        \"AUTOTHROTTLE_ENABLED\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(BookingSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get hotels' coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the json file\n",
    "\n",
    "df = pd.read_json(\"hotels.json\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce number of hotels per city to 20 as requested by the task\n",
    "for i in range(len(df[\"ville\"])):\n",
    "    df[\"hotels\"][i] = df[\"hotels\"][i][0:20]\n",
    "    df[\"liens\"][i] = df[\"liens\"][i][0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup iterates on each URL to get the hotel's score, GPS coordinates and description.\n",
    "df[\"lat\"] = 0\n",
    "df[\"lon\"] = 0\n",
    "df[\"description\"] = \"---\"\n",
    "df[\"score\"] = 0.0\n",
    "\n",
    "navigator = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1)\"\n",
    "\n",
    "for i in range(len(df[\"liens\"])):\n",
    "    lat_list = []\n",
    "    lon_list = []\n",
    "    description_list = []\n",
    "    score_list = []\n",
    "\n",
    "    hotel_list = df[\"liens\"][i]\n",
    "\n",
    "    for i2 in hotel_list:\n",
    "\n",
    "        # Sometimes BeautifulSoup doesn't manage to gather data. When it fails, it tries again.\n",
    "        try:\n",
    "            page = requests.get(i2, headers={\"User-Agent\": navigator})\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        except:\n",
    "            page = requests.get(i2, headers={\"User-Agent\": navigator})\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "        lat_list.append(\n",
    "            soup.select(\"p.address.address_clean a\")[0]\n",
    "            .get(\"data-atlas-latlng\")\n",
    "            .split(\",\")[0]\n",
    "        )\n",
    "        lon_list.append(\n",
    "            soup.select(\"p.address.address_clean a\")[0]\n",
    "            .get(\"data-atlas-latlng\")\n",
    "            .split(\",\")[1]\n",
    "        )\n",
    "        description_list.append(\n",
    "            soup.select(\"div#property_description_content\")[0].get_text()\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score_list.append(soup.select(\"div.b5cd09854e.d10a6220b4\")[0].get_text())\n",
    "\n",
    "        except:\n",
    "            # 2 hotels over the 700 that I am going to scrap dont have a score yet but I still need one for the visualization. I set it to 1.\n",
    "            score_list.append(\"1.0\")\n",
    "\n",
    "        time.sleep(1.4)\n",
    "\n",
    "    df[\"lat\"][i] = lat_list\n",
    "    df[\"lon\"][i] = lon_list\n",
    "    df[\"description\"][i] = description_list\n",
    "    df[\"score\"][i] = score_list\n",
    "\n",
    "    print(f\"city {df['ville'].iloc[i]} done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.rename(columns={\"index\": \"id\"})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ville\"] = df[\"ville\"].str.replace(\"+\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_gps_hotels.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"df_gps_hotels.csv\")\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = pd.read_csv(\"df_gps_weather.csv\")\n",
    "df_test2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your data lake using S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e23285782446c6864c996c28c3fd325702bb6068d9fb9865cc2d1c7e1d13946"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
